# pull messages from the rabbitmq info queue
input { 
  rabbitmq {
    id => "siren_senzing_updater"
    host => "${RABBITMQ_HOST}"
    queue => "senzing-rabbitmq-info-queue"
    exchange => "senzing-rabbitmq-exchange"
    user => "${RABBITMQ_USER}"
    password => "${RABBITMQ_PASSWORD}"
    passive => true
    exchange_type => "direct"
    durable => false
    key => "senzing.info"
  }
}
 
filter {
      ###################################################################################
      # aggregate events and push when num events limit (or timeout) is reached
      # requires logstash to be run with --pipeline.workers 1 --pipeline.batch.size 1
      # so that every event goes through the second aggregate filter before the first aggregate filter processes another event
      ###################################################################################
      mutate {
        add_field => { "task_id" => "senzing" }
      }
      aggregate {
        task_id => "%{task_id}"
        code => '
            map["entity_ids"] ||= []
            event.get("AFFECTED_ENTITIES").each { |e| map["entity_ids"] << e["ENTITY_ID"] }
            if map["entity_ids"].length > 1000
                event.set("[@metadata][timeToFlush]", true)
            end
        '
        push_map_as_event_on_timeout => true
        timeout => 10
        timeout_task_id_field => "task_id"
        timeout_code => '
            event.set("[@metadata][timeToFlush]", true)
        '
    }
    if [@metadata][timeToFlush] {
        aggregate {
            task_id => "%{task_id}"
            code => '
                event.set("entity_ids", map["entity_ids"])
                map["entity_ids"] = []
            '
            map_action => "update"
            end_of_task => true
        }
    } else {
        drop {}
    }
  
  # get entity details from postgres
  jdbc_streaming {
    jdbc_driver_library => "${DB_DRIVER_PATH}"
    jdbc_driver_class => "org.postgresql.Driver"
    jdbc_connection_string => "jdbc:postgresql://${DB_HOST}:5432/G2"
    jdbc_user => "${DB_USER}"
    jdbc_password => "${DB_PASSWORD}"
    statement => "SELECT REO.RES_ENT_ID AS ENTITYID, COUNT(DR.RECORD_ID) AS MATCHCOUNT, STRING_AGG(DR.RECORD_ID, E'\u241e') AS MATCHEDRECORDS, STRING_AGG(DR.JSON_DATA, E'\u241e') AS MATCHEDRECORDS_JSON FROM dsrc_record DR, OBS_ENT OE, RES_ENT_OKEY REO WHERE DR.ENT_SRC_KEY = OE.ENT_SRC_KEY AND OE.OBS_ENT_ID = REO.OBS_ENT_ID AND DR.DSRC_ID = OE.DSRC_ID AND REO.RES_ENT_ID IN :ents GROUP BY REO.RES_ENT_ID"
    parameters => { "ents" => "entity_ids"}
    target => "entity_details"
  }
 
  # get entityrels details from postgres
  jdbc_streaming {
    jdbc_driver_library => "${DB_DRIVER_PATH}"
    jdbc_driver_class => "org.postgresql.Driver"
    jdbc_connection_string => "jdbc:postgresql://${DB_HOST}:5432/G2"
    jdbc_user => "${DB_USER}"
    jdbc_password => "${DB_PASSWORD}"
    statement => "SELECT MIN_RES_ENT_ID, MAX_RES_ENT_ID, MATCH_KEY FROM RES_RELATE, RES_REL_EKEY WHERE RES_RELATE.RES_REL_ID = RES_REL_EKEY.RES_REL_ID AND RES_REL_EKEY.RES_ENT_ID IN :ents" 
    parameters => { "ents" => "entity_ids"}
    target => "entityrels_details"
  }
 
  # create an array of entity details plus entity deletes plus entityrels
  ruby {
         code => '
              # create hash of ent ids returned from postgres
              entity_details = event.get("entity_details")
              entity_details_entids = Hash.new(-1)
              event.get("entity_details").each  do  |ed|
                  entity_details_entids[ed["entityid"]] = 1
              end
 
              # create delete list of ent_ids in queue but not returned from postgres
              deletes = Hash.new(-1)
              entity_ids = event.get("entity_ids")
              entity_ids.each do |ent_id|
                  if entity_details_entids[ent_id] == -1
                     deletes[ent_id] = 1
                  end
              end
 
              # create list of entityrels_ids returned from postgres
              entityrels_details = event.get("entityrels_details")
              entityrels_ids = []
              entityrels_details.each  do  |erd|
                  entityrels_ids << erd["min_res_ent_id"].to_s + "-" + erd["max_res_ent_id"].to_s
              end
 
              # create an array of entity details plus entity deletes plus entityrels
              # later these will be split into new events and each will be sent in elastic bulk requests
              newevents = entity_details[0 .. entity_details.length()]
              deletes.each { |k, v| newevents << { "ent_id" =>  k, "delete" => true } }
              event.get("entityrels_details").each { |er| newevents << er if !er.empty? }
              event.set("newevents", newevents)
 
              # create a query to delete by query any entityrels which 
              # 1) have at least one entity in the incoming entity list that no longer exists in postgres
              # 2) have at least one entity in the incoming entity list that exists in postgres, but is not one of entityrels_ids found in postgres
 
              deletes_str = deletes.keys.join(", ")
              entity_ids_str = entity_ids.join(", ")
              entityrels_ids_str = entityrels_ids.map { |n| "\"" + n + "\"" }.join(", ")
 
              q = [ "{\"query\": {\"bool\": {\"should\": [ {\"bool\": {\"should\": [ {\"terms\":{\"min_res_ent_id\": [",
                     deletes_str,
                     "] }}, {\"terms\": {\"max_res_ent_id\": [",
                     deletes_str,
                     "]}} ]}}, {\"bool\": { \"should\": [ {\"terms\": {\"min_res_ent_id\": [",
                     entity_ids_str,
                     "]}}, {\"terms\": {\"max_res_ent_id\": [",
                     entity_ids_str,
                     "]}}], \"must_not\": {\"ids\": {\"values\": [",
                     entityrels_ids_str,
                     "]}}}}]}}}" ].join(" ")
 
              event.set("entity_rels_deletes_query", q)
 
              # tidy up
              event.remove("entity_details")
              event.remove("entityrels_details")
 
         '
  }
 
  # do the entityrels deletes
  http {
    url => "https://${ES_USER}:${ES_PASSWORD}@${ES_HOST}:9220/entityrels/_delete_by_query?ignore_unavailable=true"
    verb => "POST"
    ssl_verification_mode => "none"
    body_format => "json"
    body => '%{entity_rels_deletes_query}'
    target_body =>"entity_rels_deletes_query_response"
  }
  
  # create the new events
  split {
      field => "newevents"
  }
 
  ruby {
         code => '
            # reorganise entity delete events
            if event.get("newevents")["delete"]
               event.set("delete", true)
               event.set("entityid", event.get("newevents")["ent_id"])
            # reorganise entityrels index events
            elsif event.get("newevents")["min_res_ent_id"]
               event.get("newevents").each  do  |k, v|
                  event.set(k,v)
               end   
            # reorganise entity events and add passthrough fields
            else 
                entity_max_records = 500
                event.get("newevents").each  do  |k, v|
                    v = k=="matchedrecords" || k=="matchedrecords_json" ? (v.split("\u241e" , -1))[0, entity_max_records] : v
                    event.set(k,v)
                    if(k=="matchedrecords")
                       v.each do |mr|
                          mr_split = mr.split("|", -1)
                          new_field = mr_split[0]+"|"+mr_split[1]
                          new_field_val = event.get(new_field) ? event.get(new_field) : []
                          new_field_val << mr_split[2]
                          event.set(new_field, new_field_val)
                       end
                    end
                       
                end
 
                passthrough_fields = ENV["PASSTHROUGH_FIELDS"].split(",", -1)
                event.get("matchedrecords_json").each do |mj|
                   JSON.parse(mj).each do |k, v|
                       if passthrough_fields.include? k
                           eventval = event.get(k)
                           if eventval.nil?
                               event.set(k,v)
                           else
                               eventval = eventval.kind_of?(Array) ? eventval : [eventval]
                               recordval = v.kind_of?(Array) ? v : [v]
                               recordval.each do |rvi|
                                  if ! eventval.include? rvi
                                      eventval << rvi
                                  end
                               end
                               if eventval.length() == 1
                                   eventval = eventval[0]
                               end
                               event.set(k, eventval)
                            end
                        end
                   end
                end
            end
 
            #tidy up
            ["task_id", "entity_ids", "@version", "matchedrecords_json", 
              "@timestamp", "newevents", "entity_rels_deletes_query_response",
              "entity_rels_deletes_query"].each { |f| event.remove(f) }
 
        '
    }
}
 
# send events to the appropriate indices
output {
 if [delete] {
  elasticsearch {
      hosts => ["https://${ES_HOST}:9220"]
      index => ["entity2record"]
      user => "${ES_USER}"
      password => "${ES_PASSWORD}"
      ssl_certificate_verification => false
      action => "delete"
      document_id => "%{entityid}"
   }
  } else if [min_res_ent_id] {
      elasticsearch {
            hosts => ["https://${ES_HOST}:9220"]
            index => ["entityrels"]
            user => "${ES_USER}"
            password => "${ES_PASSWORD}"
            ssl_certificate_verification => false
            document_id => "%{min_res_ent_id}-%{max_res_ent_id}"
        }
  }
  else {
 
    elasticsearch {
      hosts => ["https://${ES_HOST}:9220"]
      index => ["entity2record"]
      user => "${ES_USER}"
      password => "${ES_PASSWORD}"
      ssl_certificate_verification => false
      document_id => "%{entityid}"
  }
  }
}

